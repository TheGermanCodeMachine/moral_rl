{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing and comparing statistics of different generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the statistics\n",
    "path_random = '..\\..\\datasets\\\\100random\\\\100\\statistics' \n",
    "random_efficiencies = pkl.load(open(path_random + \"\\effiencies.pkl\", 'rb'))\n",
    "length_cf_random = pkl.load(open(path_random + \"\\lengths_cf.pkl\", 'rb'))\n",
    "length_org_random = pkl.load(open(path_random + \"\\lengths_org.pkl\", 'rb'))\n",
    "start_points_random = pkl.load(open(path_random + \"\\start_points.pkl\", 'rb'))\n",
    "\n",
    "path_step = '..\\..\\datasets\\\\100step\\\\100\\statistics'\n",
    "step_efficiencies = pkl.load(open(path_step + \"\\effiencies.pkl\", 'rb'))[:100]\n",
    "length_cf_step = pkl.load(open(path_step + \"\\lengths_cf.pkl\", 'rb'))[:100]\n",
    "length_org_step = pkl.load(open(path_step + \"\\lengths_org.pkl\", 'rb'))[:100]\n",
    "start_points_step = pkl.load(open(path_step + \"\\start_points.pkl\", 'rb'))[:100]\n",
    "\n",
    "path_mcts = '..\\..\\datasets\\\\100mcts\\\\100\\statistics'\n",
    "mcts_efficiencies = pkl.load(open(path_mcts + \"\\effiencies.pkl\", 'rb'))\n",
    "length_cf_mcts = pkl.load(open(path_mcts + \"\\lengths_cf.pkl\", 'rb'))\n",
    "length_org_mcts = pkl.load(open(path_mcts + \"\\lengths_org.pkl\", 'rb'))\n",
    "start_points_mcts = pkl.load(open(path_mcts + \"\\start_points.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = {'validity': 1, 'proximity': 1, 'critical_state': 0.5, 'diversity': 0.5, 'realisticness': 0.2, 'sparsity': 0.5}\n",
    "with open('..\\..\\interpretability\\\\normalisation_values.pkl', 'rb') as f:\n",
    "    normalisation = pkl.load(f)\n",
    "\n",
    "mcts_prox, step_prox, random_prox = [], [], []\n",
    "mcts_val, step_val, random_val = [], [], []\n",
    "mcts_div, step_div, random_div = [], [], []\n",
    "mcts_crit, step_crit, random_crit = [], [], []\n",
    "mcts_real, step_real, random_real = [], [], []\n",
    "mcts_spar, step_spar, random_spar = [], [], []\n",
    "mcts_qc, step_qc, random_qc = [], [], []\n",
    "\n",
    "with open('..\\..\\interpretability\\logs\\qc_comparison.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.split(' ')\n",
    "        if 'validity' in line:\n",
    "            mcts_val.append(float(parts[1]) / normalisation['validity'] * weight['validity'])\n",
    "            step_val.append(float(parts[2]) / normalisation['validity'] * weight['validity'])\n",
    "            random_val.append(float(parts[3]) / normalisation['validity'] * weight['validity'])\n",
    "        elif 'diversity' in line:\n",
    "            mcts_div.append(float(parts[1]) / normalisation['diversity'] * weight['diversity'])\n",
    "            step_div.append(float(parts[2]) / normalisation['diversity'] * weight['diversity'])\n",
    "            random_div.append(float(parts[3]) / normalisation['diversity'] * weight['diversity'])\n",
    "        elif 'proximity' in line:\n",
    "            mcts_prox.append(- float(parts[1]) / normalisation['proximity'] * weight['proximity'])\n",
    "            step_prox.append(-float(parts[2]) / normalisation['proximity'] * weight['proximity'])\n",
    "            random_prox.append(-float(parts[3]) / normalisation['proximity'] * weight['proximity'])\n",
    "        elif 'critical' in line:\n",
    "            mcts_crit.append(float(parts[1]) / normalisation['critical_state'] * weight['critical_state'])\n",
    "            step_crit.append(float(parts[2]) / normalisation['critical_state'] * weight['critical_state'])\n",
    "            random_crit.append(float(parts[3]) / normalisation['critical_state'] * weight['critical_state'])\n",
    "        elif 'realistic' in line:\n",
    "            mcts_real.append(float(parts[1])   / normalisation['realisticness'] * weight['realisticness'])\n",
    "            step_real.append(float(parts[2])   / normalisation['realisticness'] * weight['realisticness'])\n",
    "            random_real.append(float(parts[3]) / normalisation['realisticness'] * weight['realisticness'])\n",
    "        elif 'sparsity' in line:\n",
    "            mcts_spar.append(float(parts[1]) / normalisation['sparsity'] * weight['sparsity'])\n",
    "            step_spar.append(float(parts[2]) / normalisation['sparsity'] * weight['sparsity'])\n",
    "            random_spar.append(float(parts[3]) / normalisation['sparsity'] * weight['sparsity'])\n",
    "\n",
    "qc_mcts = np.mean(mcts_prox) + np.mean(mcts_val) + np.mean(mcts_div) + np.mean(mcts_crit) + np.mean(mcts_real) + np.mean(mcts_spar)\n",
    "qc_step = np.mean(step_prox) + np.mean(step_val) + np.mean(step_div) + np.mean(step_crit) + np.mean(step_real) + np.mean(step_spar)\n",
    "qc_random = np.mean(random_prox) + np.mean(random_val) + np.mean(random_div) + np.mean(random_crit) + np.mean(random_real) + np.mean(random_spar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    ['statistics'] + ['mcts'] + ['step'] + ['random'],\n",
    "    ['efficiency'] + [round(np.mean(mcts_efficiencies), 2)] + [round(np.mean(step_efficiencies), 2)] + [round(np.mean(random_efficiencies), 2)],\n",
    "    ['length cf'] + [round(np.mean(length_cf_mcts), 2)] + [round(np.mean(length_cf_step), 2)] + [round(np.mean(length_cf_random), 2)],\n",
    "    ['length org'] + [round(np.mean(length_org_mcts), 2)] + [round(np.mean(length_org_step), 2)] + [round(np.mean(length_org_random), 2)],\n",
    "    ['start points'] + [round(np.mean(start_points_mcts), 2)] + [round(np.mean(start_points_step), 2)] + [round(np.mean(start_points_random), 2)],\n",
    "     ['----------'] + ['----------'] + ['----------'] + ['----------'],\n",
    "    ['validity'] + [round(np.mean(mcts_val), 2)] + [round(np.mean(step_val), 2)] + [round(np.mean(random_val), 2)],\n",
    "    ['proximity'] + [round(np.mean(mcts_prox), 2)] + [round(np.mean(step_prox), 2)] + [round(np.mean(random_prox), 2)],\n",
    "    ['diversity'] + [round(np.mean(mcts_div), 2)] + [round(np.mean(step_div), 2)] + [round(np.mean(random_div), 2)],\n",
    "    ['critical'] + [round(np.mean(mcts_crit), 2)] + [round(np.mean(step_crit), 2)] + [round(np.mean(random_crit), 2)],\n",
    "    ['realistic'] + [round(np.mean(mcts_real), 2)] + [round(np.mean(step_real), 2)] + [round(np.mean(random_real), 2)],\n",
    "    ['sparsity'] + [round(np.mean(mcts_spar), 2)] + [round(np.mean(step_spar), 2)] + [round(np.mean(random_spar), 2)],\n",
    "    ['qc'] + [round(qc_mcts, 2)] + [round(qc_step, 2)] + [round(qc_random, 2)]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "MCTS is by far the slowest. This makes sense, because it considers many more options.\n",
    "\n",
    "MCTS makes much shorter trajectories (basically always minimum length). This indicates \n",
    "- that diversity is not properly optimised for\n",
    "- the quality criteria are not traded-off well against each other\n",
    "\n",
    "The length of MCTS org and cf trajectories are the same (this is built into the method), while step and random have different lengths.\n",
    "- These are differences in when I decide to end the trajectories. A good ablation would make try to reduce these effect by e.g. ending originals of step at the same timestep as the cf\n",
    "\n",
    "For step the trajectories in the counterfactual are longer than the originals. Maybe this is built in to the method? But I think it's more likely the byproduct of some quality criteria incentivising this difference in lengths.\n",
    "\n",
    "MCTS starts slightly later than step. Random starts much later because it has uniformly distributed starting points.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
